{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Overbreath\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import segyio\n",
    "import numpy as np\n",
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Conv3D, MaxPooling3D, Conv2D, MaxPooling2D, Activation, Flatten, Dropout\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy import signal\n",
    "from random import sample\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # for cluster setting\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,6,7\"\n",
    "\n",
    "def import_process_data(state='2D'):\n",
    "    '''\n",
    "    import and process the raw data \n",
    "    state: data processed for different CNN models. \n",
    "           'RF' for random forest on P-wave\n",
    "           '2D' for 2D-CNN\n",
    "           '3D' for 3D-CNN\n",
    "    '''\n",
    "    # load 3 event sets\n",
    "    path = [\n",
    "            'CS229/Waveform_events_1A/',\n",
    "            'CS229/Waveform_events_1B/',\n",
    "            'CS229/Waveform_events_5/'\n",
    "           ]\n",
    "\n",
    "    # create a map to track unique event\n",
    "    count = 0\n",
    "    d = {}\n",
    "    for i in range(len(path)):\n",
    "        for file in os.listdir(path[i]):\n",
    "            if file.endswith(\".sgy\"):\n",
    "                name = file[:-6]\n",
    "                if name not in d:\n",
    "                    d[name] = count\n",
    "                    count += 1\n",
    "\n",
    "    labels_P = np.zeros((count,12))\n",
    "    labels_S = np.zeros((count,12))\n",
    "    data = np.zeros((count, 3, 12, 2000))\n",
    "\n",
    "    for j in range(len(path)):\n",
    "        for file in os.listdir(path[j]):\n",
    "            if file.endswith('.sgy'):\n",
    "                fn = os.path.join(path[j], file)\n",
    "                with segyio.open(fn) as f:\n",
    "                    name = file[:-6]\n",
    "                    if file[-5] == 'P':\n",
    "                        for i in range(12):\n",
    "                            labels_P[d[name], i] = 2000 - np.sum(f.trace[i])                    #P wave label\n",
    "                    elif file[-5] == 'S':\n",
    "                        for i in range(12):\n",
    "                            labels_S[d[name], i] = 2000 - np.sum(f.trace[i])                    #S wave label\n",
    "                    elif file[-5] == 'x':\n",
    "                        for i in range(12):\n",
    "                            data[d[name], 0, i] = f.trace[i] /np.amax(np.abs(f.trace[i]))       #normalize data                 \n",
    "                    elif file[-5] == 'y':\n",
    "                        for i in range(12):\n",
    "                            data[d[name], 1, i] = f.trace[i] /np.amax(np.abs(f.trace[i]))       #normalize data\n",
    "                    elif file[-5] == 'z':\n",
    "                        for i in range(12):\n",
    "                            data[d[name], 2, i] = f.trace[i] /np.amax(np.abs(f.trace[i]))       #normalize data\n",
    "\n",
    "    data = np.transpose(data, (0, 3, 2, 1))\n",
    "    \n",
    "    if state == '2D':\n",
    "        return data, labels_P, labels_S\n",
    "    \n",
    "    if state == 'RF':\n",
    "        data = np.transpose(data, (0, 2, 3, 1))\n",
    "        data = data.reshape((data.shape[0], data.shape[1], data.shape[2]*data.shape[3]))\n",
    "        data = data.reshape((-1, data.shape[2]))\n",
    "        labels_P = labels_P.flatten()\n",
    "        labels_S = labels_S.flatten()\n",
    "        return data, labels_P, labels_S\n",
    "    \n",
    "    if state == '3D':\n",
    "        data = np.transpose(data, (0, 2, 3, 1))\n",
    "\n",
    "        dt = .25e-3 # s\n",
    "        labels_P = labels_P * dt\n",
    "        labels_S = labels_S * dt\n",
    "\n",
    "        fs = 1/dt\n",
    "        window_size = 200 #filter window size for spectrogram \n",
    "        overlap = 195     #number of overlap for spectrogram\n",
    "\n",
    "        f, t, Sxx = signal.spectrogram(data[0, 0, 0, :], fs, nperseg=window_size, window=('hamming'), noverlap=overlap)\n",
    "\n",
    "        spec_data = np.zeros((data.shape[0], data.shape[1], data.shape[2], Sxx.shape[1], Sxx.shape[0]))\n",
    "        spec_P = np.zeros((data.shape[0], data.shape[1]))\n",
    "        spec_S = np.zeros_like(spec_P)\n",
    "        for i in range(data.shape[0]): # 1665\n",
    "            for j in range(data.shape[1]): # 12\n",
    "                for k in range(data.shape[2]): # 3\n",
    "                    f, t, Sxx = signal.spectrogram(data[i, j, k, :], fs, nperseg=window_size, window=('hamming'), noverlap=overlap)\n",
    "                    spec_P[i, j] = np.argmax(t >= labels_P[i, j])\n",
    "                    spec_S[i, j] = np.argmax(t >= labels_S[i, j])\n",
    "                    spec_data[i, j, k, :, :] = Sxx.T / np.amax(Sxx)\n",
    "\n",
    "        spec_data = spec_data[:,:,:,:,0:spec_data.shape[4]//4]\n",
    "        spec_data = np.transpose(spec_data, (0, 1, 3, 4, 2))\n",
    "\n",
    "        data = spec_data\n",
    "        labels_P = spec_P\n",
    "        labels_S = spec_S\n",
    "\n",
    "    return data, labels_P, labels_S\n",
    "\n",
    "def train_RF(data, labels_P, labels_S, state='P', n=1):\n",
    "    '''\n",
    "    train random forest model\n",
    "    n: number of traces to be blocked for sensitivity analysis\n",
    "    state: 'P' for P-wave\n",
    "           'S' for S-wave\n",
    "    '''\n",
    "    if state == 'P':\n",
    "        trainX, testX, trainY, testY = train_test_split(data, labels_P, test_size = 0.20)\n",
    "    if state == 'S':\n",
    "        trainX, testX, trainY, testY = train_test_split(data, labels_S, test_size = 0.20)\n",
    "        \n",
    "    model = RandomForestRegressor(n_estimators = 100, max_features = 80, min_samples_split = 12, min_samples_leaf = 5, bootstrap = True)\n",
    "    model.fit(trainX, trainY)\n",
    "    \n",
    "    predictY = model.predict(testX).flatten()\n",
    "    testY = testY.flatten()\n",
    "\n",
    "    print('test statistics:')\n",
    "    acc = np.abs(predictY - testY) <= 20\n",
    "    acc = 1. * np.sum(acc == True) / predictY.shape[0]\n",
    "    print(acc)\n",
    "    acc1 = np.abs(predictY - testY) <= 40\n",
    "    acc1 = 1. * np.sum(acc1 == True) / predictY.shape[0]\n",
    "    print(acc1)\n",
    "    acc2 = np.abs(predictY - testY) <= 80\n",
    "    acc2 = 1. * np.sum(acc2 == True) / predictY.shape[0]\n",
    "    print(acc2)\n",
    "    mse = np.sum(((predictY-testY)**2))/predictY.shape[0]\n",
    "    print(mse)\n",
    "    \n",
    "    print('sensitivity analysis:')\n",
    "    analysis(X, y, model, n, state='RF')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_2D(data, labels_P, labels_S, state='P', n=1):\n",
    "    '''\n",
    "    train 2D-CNN model\n",
    "    n: number of traces to be blocked for sensitivity analysis\n",
    "    state: 'P' for P-wave\n",
    "           'S' for S-wave\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    inputShape = (2000, 12, 3)\n",
    "\n",
    "    model.add(Conv2D(16, (5, 2), strides = (1, 1), padding=\"valid\", input_shape = inputShape, data_format=\"channels_last\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    \n",
    "    model.add(Conv2D(32, (4, 2), strides = (1, 1), padding=\"valid\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1), strides=(2,1), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(64, (2, 2), strides = (1, 1), padding=\"valid\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "\n",
    "    model.add(Conv2D(128, (2, 2), strides = (1, 1), padding=\"valid\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1), strides=(2,1), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(108))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(36))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(12))\n",
    "\n",
    "    EPOCHS = 120\n",
    "    INIT_LR = 1e-2\n",
    "    BS = 128\n",
    "\n",
    "    opt = keras.optimizers.adam(lr = INIT_LR)\n",
    "    model.compile(optimizer = opt, loss = 'mse')\n",
    "\n",
    "    if state == 'P':\n",
    "        trainX, testX, trainY, testY = train_test_split(data, labels_P, test_size = 0.15)\n",
    "    if state == 'S':\n",
    "        trainX, testX, trainY, testY = train_test_split(data, labels_S, test_size = 0.15)\n",
    "\n",
    "    # keras.callbacks.CSVLogger(filename, separator=',', append=False)\n",
    "    csv_logger = CSVLogger('training.log')\n",
    "\n",
    "    model.fit(x=trainX, y=trainY, batch_size=BS,\n",
    "                                    validation_split=0.18,\n",
    "                                    callbacks=[csv_logger],\n",
    "                                    epochs=EPOCHS,\n",
    "                                    shuffle=True)\n",
    "\n",
    "    #print(model.summary)\n",
    "    predictY = model.predict(testX).flatten()\n",
    "    testY = testY.flatten()\n",
    "\n",
    "    print('test statistics:')\n",
    "    acc = np.abs(predictY - testY) <= 20\n",
    "    acc = 1. * np.sum(acc == True) / predictY.shape[0]\n",
    "    print(acc)\n",
    "    acc1 = np.abs(predictY - testY) <= 40\n",
    "    acc1 = 1. * np.sum(acc1 == True) / predictY.shape[0]\n",
    "    print(acc1)\n",
    "    acc2 = np.abs(predictY - testY) <= 80\n",
    "    acc2 = 1. * np.sum(acc2 == True) / predictY.shape[0]\n",
    "    print(acc2)\n",
    "    mse = np.sum(((predictY-testY)**2))/predictY.shape[0]\n",
    "    print(mse)\n",
    "    \n",
    "    print('sensitivity analysis:')\n",
    "    analysis(X, y, model, n, state='2d')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_3D(data, labels_P, labels_S, state='P', n=1):\n",
    "    '''\n",
    "    train 3D-CNN model\n",
    "    n: number of traces to be blocked for sensitivity analysis\n",
    "    state: 'P' for P-wave\n",
    "           'S' for S-wave\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    inputShape = (data.shape[1], data.shape[2], data.shape[3], data.shape[4])\n",
    "    \n",
    "    model.add(Conv3D(16, (2, 4, 4), strides = (1, 1, 1), padding=\"valid\", input_shape = inputShape, data_format=\"channels_last\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv3D(32, (2, 3, 3), strides = (1, 1, 1), padding=\"valid\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv3D(64, (2, 2, 2), strides = (1, 1, 1), padding=\"valid\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(108))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(36))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(12))\n",
    "\n",
    "    EPOCHS = 120\n",
    "    INIT_LR = 1e-2\n",
    "    BS = 128\n",
    "    \n",
    "    opt = keras.optimizers.adam(lr = INIT_LR)\n",
    "    model.compile(optimizer = opt, loss = 'mse')\n",
    "\n",
    "    if state == 'P':\n",
    "        trainX, testX, trainY, testY = train_test_split(data, labels_P, test_size = 0.15)\n",
    "    if state == 'S':\n",
    "        trainX, testX, trainY, testY = train_test_split(data, labels_S, test_size = 0.15)\n",
    "\n",
    "\n",
    "    # keras.callbacks.CSVLogger(filename, separator=',', append=False)\n",
    "    csv_logger = CSVLogger('TF_12_S.log')\n",
    "\n",
    "    model.fit(x=trainX, y=trainY, batch_size=BS,\n",
    "                                    validation_split=0.18,\n",
    "                                    callbacks=[csv_logger],\n",
    "                                    epochs=EPOCHS,\n",
    "                                    shuffle=True)\n",
    "\n",
    "    predictY = model.predict(testX).flatten()\n",
    "    testY = testY.flatten()\n",
    "    \n",
    "    print('test statistics:')\n",
    "    acc = np.abs(predictY - testY) <= 8 /2000 * 361\n",
    "    acc = 1. * np.sum(acc == True) / predictY.shape[0]\n",
    "    print(acc)\n",
    "    acc1 = np.abs(predictY - testY) <= 20 /2000 * 361\n",
    "    acc1 = 1. * np.sum(acc1 == True) / predictY.shape[0]\n",
    "    print(acc1)\n",
    "    acc2 = np.abs(predictY - testY) <= 80 /2000 * 361\n",
    "    acc2 = 1. * np.sum(acc2 == True) / predictY.shape[0]\n",
    "    print(acc2)\n",
    "    mse = np.sum(((predictY-testY)**2))/predictY.shape[0]\n",
    "    print(mse)\n",
    "    \n",
    "    print('sensitivity analysis:')\n",
    "    analysis(X, y, model, n, state='3d')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def analysis(X, y, model, n, state='2D'):\n",
    "    '''\n",
    "    sensitivity analysis\n",
    "    only for 2D or 3D CNN\n",
    "    n: the number of traces to be blocked\n",
    "    state: '2D' for 2D-CNN\n",
    "           '3D' for 3D-CNN\n",
    "    '''\n",
    "    traces = [i for i in range(12)]\n",
    "    block = sample(traces, n)\n",
    "    k = 1   #coefficient for different time scale\n",
    "    if state == '2D':\n",
    "        for i in range(12):\n",
    "            if i in block:\n",
    "                X[:,:,i,:] = 0\n",
    "    if state == '3D':\n",
    "        k = 361/2000\n",
    "        for i in range(12):\n",
    "            if i in block:\n",
    "                X[i,:,:,:,:] = 0\n",
    "    \n",
    "    predictY = model.predict(testX).flatten()\n",
    "    testY = y.flatten()\n",
    "\n",
    "    acc = np.abs(predictY - testY) <= 20 * k\n",
    "    acc = np.sum(acc == True) / predictY.shape[0] *1.0\n",
    "    print(acc)\n",
    "    acc1 = np.abs(predictY - testY) <= 40 * k\n",
    "    acc1 = 1. * np.sum(acc1 == True) / predictY.shape[0]\n",
    "    print(acc1)\n",
    "    acc2 = np.abs(predictY - testY) <= 80 * k\n",
    "    acc2 = 1. * np.sum(acc2 == True) / predictY.shape[0]\n",
    "    print(acc2)\n",
    "    mse = np.sum(((predictY-testY)**2))/predictY.shape[0]\n",
    "    print(mse)\n",
    "    \n",
    "def main(state='2D', wave='P', n):\n",
    "    '''\n",
    "    load and process data, train and test model, do sensitivity analysis\n",
    "    state: 'RF' for random forest\n",
    "           '2D' for 2D-CNN\n",
    "           '3D' for 3D-CNN\n",
    "    wave: 'P' for P-wave\n",
    "          'S' for S-wave\n",
    "    n: the number of traces to be blocked\n",
    "    '''\n",
    "    data, labels_P, labels_S = import_process_data(state)\n",
    "    if state == 'RF':\n",
    "        model = train_RF(data, labels_P, labels_S, wave, n)\n",
    "    if state == '2D':\n",
    "        model = train_2D(data, labels_P, labels_S, wave, n)\n",
    "    if state == '3D':\n",
    "        model = train_3D(data, labels_P, labels_S, wave, n)\n",
    "    \n",
    "main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
